{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "## 1 Different Parsing techniques\n",
    "\n",
    "In this assignment, we found that the texts are possible derived from a script called *Monty Python and the Holy Grail*. A reasonable way to utilize this extra text source is parsing it with state-of-the-art English parser to generate parsed grammar trees, and traverse these trees to generate grammar files. \n",
    "\n",
    "Parsing English on Penn Treebank dataset has been well studied by previous researchers. Researchers from Stanford has proposed a method to train highly accurate praser over Penn Treebank dataset[1]. They have packed their software into jar, and distributed on their website. In their parser package , there are two parsers, one traditional statistical PCFG parser, and a RNN based PCFG parser. Both parsers are trained with Penn Treebank dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Generate S1.gr & Vocab.gr\n",
    "\n",
    "### 2.1 Generate trees\n",
    "\n",
    "* Use provided devset.trees\n",
    "\n",
    "* Use state-of-the-art parser to parse existed materials. \n",
    "\n",
    "### 2.2 Generate grammars files from grammar trees\n",
    "\n",
    "\n",
    "1. Given grammar trees(e.g. `devset.trees`) parse it to seperate each parenthesis surrounded strings.\n",
    "\n",
    "    * For given `devset.trees`, the parenthesis are not sanitized.(e.g. `(( ()` which implies `( -> (`) This would result in failure of later processing. So we identified such cases in the grammar trees and sanitize '(' and ')' to '-LRB-' and '-RRB-' as in the result of stanford parser. \n",
    "    \n",
    "2. Construct the tree by using `nltk.Tree.fromstring()`\n",
    "\n",
    "3. Convert the tree to CNF by using `nltk.Tree.chomsky_normal_form()`\n",
    "\n",
    "4. Traverse the tree by depth first search. Use a dictionary to keep counting the frequency of one symbol followed by another.\n",
    "\n",
    "    * In the grammer trees, there are rules like `. -> .`, `, -> ,` where '.' and ',' are both nonterminal and ternimal. To address this problem, we sanitize nonterminal punctuations to things like 'PERIOD' 'COMMA'. And because of we previously sanitized '(' and ')' to '-LRB-' and '-RRB-', which results in rule like `-LRB- -> -LRB-`. So we sanitize the termial '-LRB-' and '-RRB-' to '(' and ')'.\n",
    "    \n",
    "    * In short, We make sure every non terminals are uppercase letters and there is no symbols that are both non terminal and terminal.\n",
    "    \n",
    "5. Output the grammar weighted by frequence to output grammar files `S1.gr` and `Vocab.gr`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.3 Add other unseen words with tags\n",
    "\n",
    "After generating `S1.gr` and `Vocab.gr` using grammar trees, we found that not every word in `allowed_words.txt` appears in `Vocab.gr`. Thus we must handle those unseen words with some meaningful tags.\n",
    "\n",
    "`nltk` can assign POS tag for any single word by \n",
    "\n",
    "```python\n",
    "nltk.tag.pos_tag([word])\n",
    "```\n",
    "\n",
    "So we can simply iterate all the words in `allowed_words.txt`, if that word doesn’t exist in `Vocab.gr`, we assign it with tag and append it to the end of file.\n",
    "The code that generates unseen words, assigns tags and put them into `Vocab.gr` is in `vocab_generator.py`.\n",
    "\n",
    "Note that one word may have multiple POS tags in different context, however we didn’t find a way to list all the possible tags. Also we only have small percentage unseen words, so we think it is ok to not include all tags of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Generate S2.gr\n",
    "\n",
    "### 3.1 Unigram\n",
    "\n",
    "As mentioned above, Vocab.gr maps tags to words like this:\n",
    "|Weight|Tag|Word|\n",
    "|---|---|---|\n",
    "|1|NNP|Whoa|\n",
    "|18|NNP|Arthur|\n",
    "|1|NNP|Uther|\n",
    "|6|EX|there|\n",
    "\n",
    "Our first approach to generate S2.gr is to simply generate a unigram. We want S2 to accept sentences that consist of any random words in any random order, and the occurrence of a word is completely independent of other words in that same sentence. In other words, the probability that one tag follows another is flat. Without taking the relationship between tags into account, we simply calculate: how often a tag occurs, i.e. the amount of its occurrence in Vocab.gr. That being said, the above Vocab.gr produces the following S2.gr:\n",
    "\n",
    "|Weight|Left|Right|Comment|\n",
    "|---|---|---|---|\n",
    "|1|S2|_Word|# S2 consists of any length of Words|\n",
    "|1|_Word|Word _Word|\n",
    "|1|_Word|Word|\n",
    "|3|Word|NNP|# There is 3 NNPs in Vocab.gr|\n",
    "|1|Word|EX|# There is 1 EX in Vocab.gr|\n",
    "\n",
    "\n",
    "### 3.2 Bigram\n",
    "\n",
    "In the bigram approach, unlike the unigram approach, we consider the relationship between tags. We first initilize all possible combinations  We calculate the number of times a tag is the beginning of a sentence, is the end of a sentence and follows another tag.\n",
    "\n",
    "Say we only have this one sentence:\n",
    "\n",
    "`(INTJ (NNP Whoa) (ADVP (EX there)))`\n",
    "\n",
    "S2.gr will look like:\n",
    "\n",
    "|Weight|Left|Right|Comment|\n",
    "|---|---|---|---|\n",
    "|2|S2|_NNP|# Each rule starts with an initial value of 1. NNP is the beginning of 1 sentence so here the weight is 1+1=2|\n",
    "|1|S2|_EX||\n",
    "|1|_NNP|NNP|# A sentence ends with NNP|\n",
    "|1|_NNP|NNP _NNP|# NNP is followed by another NNP|\n",
    "|2|_NNP|NNP _EX|\n",
    "|2|_EX|EX|# A sentence ends with EX|\n",
    "|1|_EX|EX _EX|\n",
    "|1|_EX|EX _NNP|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "## 1 Compare different grammars from different training set\n",
    "\n",
    "Based on the methods we talked above, we want to compare performance on different training set.\n",
    "We prepared the following training set:\n",
    "\n",
    "`devset.txt` only\n",
    "`devset.txt` + `more_sentences.txt`\n",
    "`devset.txt` + `example_sentences.txt`\n",
    "`devset.txt` + `example_sentences.txt` + `more_sentences.txt`\n",
    "\n",
    "*Note: where more sentences come from?\n",
    "We realize that the devset sentences all come from Monty Python and the Holy Grail, which nltk.books has. We get all the sentences from the book, reformat them so that they match the style as in devset.text (e.g. transform “couldn ’ t” to “could n’t”). Then we keep only the sentences consisting of only allowed words.*\n",
    "\n",
    "| Training data \\ Test data                   | example sentences | more example sentences |\n",
    "| ------------------------------------------- | ----------------- | ---------------------- |\n",
    "| devset                                      | -8.92096          | -9.57106               |\n",
    "| devset + example sentences                  | -7.87721          | -8.83074               |\n",
    "| devset + more sentences                     | -9.03043          | -9.67019               |\n",
    "| devset + more sentences + example sentences | -8.31589          | -9.04387               |\n",
    "\n",
    "\n",
    "Since our test data will come from the same distribution of `devset.txt` & `example_sentences.txt`, we need to make sure we fully utilize it in our training.\n",
    "So we choose to use `devset.txt` & `example_sentences.txt` as training data in out final grammars.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pcfg_parse_gen import Pcfg, PcfgGenerator, CkyParse\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_grammar(s1, s2, vocab, test):\n",
    "    parse_gram = Pcfg([s1, s2, vocab])\n",
    "    parser = CkyParse(parse_gram, beamsize=0.0001, verbose=0)\n",
    "    ce, trees = parser.parse_file(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On `text/example_sentences.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#-cross entropy (bits/word): -8.66895\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/devset_s1.gr\", \"grammars/devset_s2.gr\", \"grammars/devset_vocab.gr\", \"text/example_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#No parses found for: they migrate precisely because they know they will grow .\n",
      "#-cross entropy (bits/word): -13.819\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/devset_rnn_s1.gr\", \"grammars/devset_rnn_s2.gr\", \"grammars/devset_rnn_vocab.gr\", \"text/example_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#-cross entropy (bits/word): -9.15619\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/devset_stanford_s1.gr\", \"grammars/devset_stanford_s2.gr\", \"grammars/devset_stanford_vocab.gr\", \"text/example_sentences.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On `text/more_examples.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#-cross entropy (bits/word): -9.43403\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/devset_s1.gr\", \"grammars/devset_s2.gr\", \"grammars/devset_vocab.gr\", \"text/more_examples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#No parses found for: they migrate precisely because they know they will grow .\n",
      "#-cross entropy (bits/word): -11.2527\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/devset_rnn_s1.gr\", \"grammars/devset_rnn_s2.gr\", \"grammars/devset_rnn_vocab.gr\", \"text/more_examples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#-cross entropy (bits/word): -9.70726\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/devset_stanford_s1.gr\", \"grammars/devset_stanford_s2.gr\", \"grammars/devset_stanford_vocab.gr\", \"text/more_examples.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2 Compare different parsers on devset\n",
    "\n",
    "Since we also found some other parsers, we want to know which one is better. Here is our test result:\n",
    "\n",
    "| Parser \\ Test data    | example sentences | more example sentences |\n",
    "| --------------------- | ----------------- | ---------------------- |\n",
    "| provided devset.trees | -8.92096          | -9.57106               |\n",
    "| stanford parser       | -9.15619          | -9.70726               |\n",
    "| rnn parser            | -13.819           | -11.2527               |\n",
    "\n",
    "\n",
    "We finally found that the `devset.trees` provided in the repo generate the best entropy. Thus we will use that as grammar trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On `text/example_sentences.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#-cross entropy (bits/word): -8.66895\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/devset_s1.gr\", \"grammars/devset_s2.gr\", \"grammars/devset_vocab.gr\", \"text/example_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#-cross entropy (bits/word): -7.87721\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/devset_and_examplesentences_s1.gr\", \"grammars/devset_and_examplesentences_s2.gr\", \"grammars/devset_and_examplesentences_vocab.gr\", \"text/example_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#-cross entropy (bits/word): -9.03043\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/moresentences_devset_s1.gr\", \"grammars/moresentences_devset_s2.gr\", \"grammars/moresentences_devset_vocab.gr\", \"text/example_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#-cross entropy (bits/word): -8.31589\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/moresentences_devset_examplesentences_s1.gr\", \"grammars/moresentences_devset_examplesentences_s2.gr\", \"grammars/moresentences_devset_examplesentences_vocab.gr\", \"text/example_sentences.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On `text/more_examples.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#-cross entropy (bits/word): -9.43403\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/devset_s1.gr\", \"grammars/devset_s2.gr\", \"grammars/devset_vocab.gr\", \"text/more_examples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#-cross entropy (bits/word): -8.83074\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/devset_and_examplesentences_s1.gr\", \"grammars/devset_and_examplesentences_s2.gr\", \"grammars/devset_and_examplesentences_vocab.gr\", \"text/more_examples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#-cross entropy (bits/word): -9.67019\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/moresentences_devset_s1.gr\", \"grammars/moresentences_devset_s2.gr\", \"grammars/moresentences_devset_vocab.gr\", \"text/more_examples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#-cross entropy (bits/word): -9.04387\n"
     ]
    }
   ],
   "source": [
    "test_grammar(\"grammars/moresentences_devset_examplesentences_s1.gr\", \"grammars/moresentences_devset_examplesentences_s2.gr\", \"grammars/moresentences_devset_examplesentences_vocab.gr\", \"text/more_examples.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3 Compare different methods to generate S2\n",
    "\n",
    "We examine S2 by only using S2.gr and Vocab.gr for parsing.\n",
    "\n",
    "Beamsize: 0.0001\n",
    "\n",
    "|S2.gr|Trained on|Entropy on example sentences|Entropy on devset|\n",
    "|-----|-----|-----|-----|\n",
    "|s2_unigram.gr|devset| -11.1758|-11.2044|\n",
    "|S2_bigram.gr|devset|-33.2766|-13.6859|\n",
    "\n",
    "Beamsize: 0.000001\n",
    "\n",
    "|S2.gr|Trained on|Entropy on example sentences|Entropy on devset|\n",
    "|-----|-----|-----|-----|\n",
    "|s2_unigram.gr|devset| ?| ?|\n",
    "|S2_bigram.gr|devset|?|?|\n",
    "\n",
    "We found that for bigram works better when beam size is small enough. That is expected result since we have the frequency of one word follows another word and thus the entropy is better.\n",
    "\n",
    "However when beam size is 0.0001, some sentences may fail to be passed due to pruning inside the praser. For those sentences failed, the entropy is super bad. \n",
    "\n",
    "We compared \n",
    "\n",
    "In order to make sure we parse every sentences successfully we choose to use unigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "According to the experiments we did, we chose the grammar trained on `devset` and `more_sentences`. Although the test result is slightly worse that using only `devset`, we think that this maybe because the test data set is small and trained on more data would generalize better for unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-cross entropy: -9.03042669531999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#-cross entropy (bits/word): -9.03043\n"
     ]
    }
   ],
   "source": [
    "from pcfg_parse_gen import Pcfg, PcfgGenerator, CkyParse\n",
    "import nltk\n",
    "\n",
    "parse_gram = Pcfg([\"grammars/devset_more_s1.gr\",\"grammars/devset_more_s2.gr\",\"grammars/devset_more_vocab.gr\"])\n",
    "parser = CkyParse(parse_gram, beamsize=0.0001, verbose=0)\n",
    "ce, trees = parser.parse_file('text/example_sentences.txt')\n",
    "print(\"-cross entropy: {}\".format(ce))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# References\n",
    "[1] Danqi Chen and Christopher D Manning. 2014. *A Fast and Accurate Dependency Parser using Neural Networks.* Proceedings of EMNLP 2014\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
