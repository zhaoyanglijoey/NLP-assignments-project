{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this homework we tried two models for alignment:\n",
    "\n",
    "- IBM Model 1\n",
    "- HMM model\n",
    "\n",
    "And we apply bidirectional alignment training and decoding for both models. We found that by using the pretrained parameters in IBM model 1 for HMM model, we got relatively nice **AER -- 0.107** after 5 iterations.\n",
    "\n",
    "# Baseline (IBM model 1)\n",
    "\n",
    "For each English-French word pair, we have $t(f|e)$, which is initially $1/|f|$.\n",
    "\n",
    "For each iteration, we:\n",
    "\n",
    "* initial count() and count_pair() to 0\n",
    "* for each parallel sentence pair $(f,e)$\n",
    " * for each French word $f_i$\n",
    " * $z = \\sum_{e_j} t(f_i|e_j)$\n",
    "  * for each English word $e_j$\n",
    "   * $c = t(f_i|e_j) / z$\n",
    "   * $count\\_pair(f_i|e_j) += c$\n",
    "   * $count(e_j) += c$\n",
    "* for each word pair (f,e) in count_pair()\n",
    " * $t(f|e) = count\\_pair(f|e) / count_e(e)$\n",
    "\n",
    "Repeat the process until the difference between the new log likelihood and the previous one is smaller than a fixed value epsilon or until we have run a fixed number of iterations.\n",
    "\n",
    "### Result\n",
    "\n",
    "After 8 iterations:\n",
    "* Precision = 0.599407\n",
    "* Recall = 0.773403\n",
    "* AER = 0.341046\n",
    "\n",
    "# Improvements\n",
    "\n",
    "## Bidirectional IBM model 1 (align using $Pr(f|e)$ and $Pr(e|f)$)\n",
    "\n",
    "Align using $Pr(f|e)$ and also align using $Pr(e|f)$. Then decode the best alignment using each model independently. Then report the alignments that are the intersection of these two alignment sets.\n",
    "\n",
    "### Result\n",
    "\n",
    "After 100 iterations:\n",
    "* Precision = 0.867216\n",
    "* Recall = 0.695146\n",
    "* AER = 0.220469\n",
    "\n",
    "### Analysis\n",
    "\n",
    "We can see that by intersecting the decoding results of two alignment directions, we got much higher precision but lower recall. This means we discarded many good results which do not appear in the intersections. There are ways to improve both precision and recall by intersecting during training (Liang et al.[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM-based alignment model\n",
    "\n",
    "## Baseline\n",
    "\n",
    "HMM alignment model is a extension to IBM model 1 which models not only emittion probability $p \\left( f _ { j } | e _ { a _ { j } } \\right)$ but also transition probability $p \\left( a _ { j } | a _ { j - 1 } , I \\right)$ as follows,\n",
    "\n",
    "$$\n",
    "     Pr( f | e ) = \\sum _ { a } \\prod _ { j = 1 } ^ { J } \\left[ p \\left( a _ { j } | a _ { j - 1 } , I \\right) \\cdot p \\left( f _ { j } | e _ { a _ { j } } \\right) \\right]\n",
    "$$\n",
    "\n",
    "Train can be done using the Baum-Welch[7] algorithm which makes use of the forward-backward algorithm[3]. The parameters are re-estmated as,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p ( f | e ) &= \\frac { c ( f , e ) } { \\sum _ { f ^ { \\prime } } c \\left( f ^ { \\prime } , e \\right) } \\\\\n",
    "p ( i | i ^ { \\prime } , I ) &= \\frac { c \\left( i ^ { \\prime } , i , I \\right) } { \\sum _ { i ^ { \\prime } = 1 } ^ { I } c \\left( i ^ { \\prime } , i ^ { \\prime \\prime } , I \\right) }\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Viterbi decoding can be applied to decode. See[3] for more detail.\n",
    "\n",
    "### Result\n",
    "\n",
    "After 4 iterations:\n",
    "* Precision = 0.731220\n",
    "* Recall = 0.862803\n",
    "* AER = 0.223748\n",
    "\n",
    "\n",
    "\n",
    "## Extensions\n",
    "\n",
    "### Smoothing\n",
    "\n",
    "Following [4] we add smoothing to transition probability,\n",
    "\n",
    "$$p\\left( a _ { j } | a _ { j - 1 } , I \\right) = \\alpha \\cdot \\frac { 1 } { I } + ( 1 - \\alpha ) \\cdot p \\left( a _ { j } | a _ { j - 1 } , I \\right)\n",
    "$$\n",
    "\n",
    "$\\alpha$ is set to 0.4 as in [4] in our experiments.\n",
    "\n",
    "### Word-Dependent Transition Model\n",
    "\n",
    "Baseline HMM model only models transition probability given the previous alignment. He[5] used a transition probability that is word dependent. The Word-Dependent HMM Model is therefore,\n",
    "\n",
    "$$\n",
    "Pr( \\boldsymbol { f } | \\boldsymbol { e } ) = \\sum _ { a } \\prod _ { j = 1 } ^ { J } \\left[ p \\left( a _ { j } | a _ { j - 1 } , e _ { a _ { j - 1 } } , I \\right) \\cdot p \\left( f _ { j } | e _ { a _ { j } } \\right) \\right]\n",
    "$$\n",
    "\n",
    "Data sparsity is often a problem for word dependent transition probability. To estimate $p ( i | i ^ { \\prime } , e , I )$, maximum a posteriori (MAP) is used,\n",
    "\n",
    "$$\n",
    "p _ { M A P } ( i | i ^ { \\prime } , e , I ) = \\frac { c \\left( i - i ^ { \\prime } ; e \\right) + \\tau \\cdot p ( i | i ^ { \\prime } , I ) } { \\sum _ { i ^ { \\prime \\prime } = 1 } ^ { I } c \\left( i ^ { \\prime \\prime } - i ^ { \\prime } ; e \\right) + \\tau }\n",
    "$$\n",
    "\n",
    "Following [5], we set $\\tau$ to 1000 in practice.\n",
    "\n",
    "### Distortion in Buckets\n",
    "\n",
    "When calculating distortion count $c \\left( i - i ^ { \\prime } , I \\right)$ and $c \\left( i - i ^ { \\prime } ; e \\right)$, the counts are put into buckets $c[d<=-7], c[d=-6], ..., c[d>=7]$ following the idea in [2] and settings in [5]. The counts for $d<=-7$ and $d>=7$ are evenly distributed when estimating parameters.\n",
    "\n",
    "### Modelling Fertility\n",
    "\n",
    "As in [6], we further extend our model to model fertility. \n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\tilde { p } \\left( a _ { j } | a _ { j - 1 } , e _ { a _ { j - 1 } } , I \\right) & = \\delta \\left( a _ { j } , a _ { j - 1 } \\right) p ( stay | e _ { a _ { j - 1 } } ) \\\\ & + \\left( 1 - \\delta \\left( a _ { j } , a _ { j - 1 } \\right) \\right) ( 1 - p(stay | e _ { a _ { j - 1 } } ) ) p \\left( a _ { j } | a _ { j - 1 } , I \\right) \\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "p ( stay | e _ { a _ { j - 1 } } ) = \\lambda p _ { Z J } + ( 1 - \\lambda ) p (stay | e _ { a _ { j - 1 } } )\n",
    "$$\n",
    "\n",
    "and $p _ { Z J } = Pr \\left( a _ { j } = i | a _ { j - 1 } = i , I \\right)$, $\\delta \\left( a _ { j } , a _ { j - 1 } \\right)$ is the Kronecker delta function.\n",
    "\n",
    "$\\lambda$ is set to 0.1 as in [6]. \n",
    "\n",
    "### Bidirection\n",
    "\n",
    "Similar to what we did on IBM model 1, we make HMM bidirectional.\n",
    "\n",
    "## Combine bidirectional HMM model with bidirectional IBM model 1\n",
    "\n",
    "We first train bidirectional IBM model 1 for 10 iteration and then use the parameters for bidirectional HMM model. Note that HMM model is a extension to IBM model 1. They both model the same emittion(translation) probability $p(f_j | e_i)$. Traing bidirectional HMM is very slow. it take around one and a half hour to train 1 iteration. We train it for 5 iteration. As we can see from the experiment results with or without loading pretrained IBM model 1, loading pretrained IBM model increases the result significantly.\n",
    "\n",
    "### Result without pretrained IBM 1\n",
    "\n",
    "After 5 iteration:\n",
    "* Precision = 0.952381\n",
    "* Recall = 0.750619\n",
    "* AER = 0.150448\n",
    "\n",
    "### Result with pretrained IBM 1\n",
    "\n",
    "After 5 iteration:\n",
    "* Precision = 0.954314\n",
    "* Recall = 0.826895\n",
    "* AER = 0.107305\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] \"IBM Models\". SMT Research Survey Wiki. 11 September 2015. Retrieved 20 Nov 2018.\n",
    "\n",
    "[2]  P. Liang, B. Taskar, and D. Klein. Alignment by agreement. In NAACL. 2006\n",
    "\n",
    "[3] Word Alignment for Statistical Machine Translation Using Hidden Markov Models. Anahita Mansouri Bigvand. 2015\n",
    "\n",
    "[4] Franz Josef Och and Hermann Ney. A comparison of alignment models for statistical machine translation. In *Proceedings of the 18th conference on Computational linguistics- Volume 2*, pages 1086–1090. Association for Computational Linguistics, 2000a.\n",
    "\n",
    "[5] Xiaodong He. Using word dependent transition models in hmm based word alignment for statistical machine translation. In *Proceedings of the Second Workshop on Statistical Machine Translation*, pages 80–87. Association for Computational Linguistics, 2007.\n",
    "\n",
    "[6] Kristina Toutanova, H Tolga Ilhan, and Christopher D Manning. Extensions to hmm- based statistical word alignment models. In *Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10*, pages 87–94. Association for Computational Linguistics, 2002.\n",
    "\n",
    "[7] Leonard E Baum. An equality and associated maximization technique in statistical estima- tion for probabilistic functions of markov processes. *Inequalities*, 3:1–8, 1972."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision = 0.954314\n",
      "Recall = 0.826895\n",
      "AER = 0.107305\n"
     ]
    }
   ],
   "source": [
    "import argparse, sys, os, logging\n",
    "from itertools import islice\n",
    "import pickle\n",
    "from HMMmodel import BiHMMmodel, score_alignments\n",
    "\n",
    "f_data = \"data/hansards.fr\"\n",
    "e_data = \"data/hansards.en\"\n",
    "a_data = \"data/hansards.a\"\n",
    "with open(f_data) as f, open(e_data) as e, open(a_data) as a:\n",
    "    f_data, e_data, a_data = f.readlines(),\\\n",
    "                             e.readlines(), \\\n",
    "                             a.readlines()\n",
    "\n",
    "bitext = [[sentence.strip().split() for sentence in pair] for pair in \n",
    "    zip(f_data, e_data)]\n",
    "rev_bitext = [[e_sentence, f_setence] for f_setence, e_sentence in bitext]\n",
    "bihmmmodel = BiHMMmodel()\n",
    "bihmmmodel.load_model('bihmm_iter5.m')\n",
    "bihmmmodel.validate(bitext, rev_bitext, f_data, e_data, a_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
