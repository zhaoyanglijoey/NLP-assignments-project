{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Baseline (Nuhn, 2013)\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Beamsize decay\n",
    "\n",
    "In experiments we found that the correct result are likely to be pruned in early stage and in later stage where more ciphers are deciphered correct result usually gets a better score. So in order to better utilize computing resource and improve efficency we implemented beamsize decay. For the first half of the ciphers, the beamsize remain unchanged. For the second half, the beamsize are decayed exponentially by a factor of 0.85.\n",
    "\n",
    "``` python\n",
    "def dynamic_beamsize(cipher, beamsize):\n",
    "    num_symbols = len(set(cipher))\n",
    "    beamsizes = [beamsize] * (num_symbols)\n",
    "    for i in range(num_symbols // 2, num_symbols):\n",
    "        beamsizes[i] = int(beamsize * (0.85 ** (i - num_symbols//2)))\n",
    "    return beamsizes\n",
    "\n",
    "```\n",
    "\n",
    "### 2.3 Multiprocessing\n",
    "\n",
    "In order to improve efficency, we implemented multiprocessing. When scoring the partially deciphered string, instead of scoring it as a whole using `score_bitstring`, we split it into non contiguous substrings and use multiple process to score each of them. We use different scoring function for begin, end, and middle substrings to include to probabilty of start and end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Igoring unigrams and bigrams\n",
    "\n",
    "At first we score every substring using the 6-gram language model. We examined in each iteration whether the correct mappings is pruned or not by printing out the worst score in the beams and the score for correct mappings(only a subset of ciphers that is already searched). If the correct score is better than the worst score, it means the correct mappings is still in the beams. And we found that in the early stage where few ciphers are deciphered(where the partially deciphered text is full of unigrams and bigrams), the score for the correct mappings is very bad, which makes it be pruned very early. And later searching leads to very bad result because it starts off based on very wrong mappings. \n",
    "\n",
    "So we tried to ignore uingrams and bigrams when scoring. We tried to ignore unigrams only and the correct result are less easier to be pruned but still not good enough. If ignoring both unigrams and bigrams, we need to make the beamsize sufficiently large at the start because the scores are all 0 for the first few ciphers. \n",
    "\n",
    "We are still running the program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
