{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Motivation\n",
    "\n",
    "The emergence and ubiquitousness of the social media like Facebook, Twitter and Instagram have given each individual a platform to connect with people and also speak for himself/herself. People are becoming more and more used to expressing their thoughts though social media which have already become a \"We media\". People are also affected by other people's comments and opinions, which could be heard by more through social media. Interesingly also, one could get a sense of whether a certain product, organization or people is liked or not by monitoring social media/forums. In fact, Companies are already taking steps of managing branding or reputation over the internet.\n",
    "\n",
    "In the mean time, the widely use of social medias have also provided with researchers a large amount of data and interesting topics to dive into, for example social network analysis. Twitter, in particular, being a platform where people make comments, produces tons of natural language data everyday. Utilizing NLP and machine learning algorithms, we could do many intersting things with the tweets people posted. In this project, we are interested in analyzing the reputation of certain entities by sentiment analysis on tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Related Work\n",
    "\n",
    "## 2.1 Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is a well studied area of Natural Language Processing (NLP). Just in the past year, there have been a number of papers looking at sentiment analysis on different datasets [10; 11; 12]. In traditional ways of sentiment analysis, this task was usually tackled using hand-crafted features or sentiment lexicons [13; 14], feeding them to classifiers such as Naive Bayes or Support Vector Machines (SVM). These methods require a laborious feature engineering process and may require domain-specific knowledge, often resulting in redundant and missing features. With the recent development of deep learning, more solutions [3; 4; 5] utilizing deep learning and neural networks based on Deep Recurrent Neural Network (RNN) or Long Short-Term Memory (LSTM) achieved very good results compared with traditional methods. The sentiment analysis in Twitter has also been a annual task run by SemEval since 2013 [3; 8; 9]. In our project we ran the SemEval task as comparison and benchmarking. It is shown that our model outperforms the state-of-the-art method in SemEval 2017.\n",
    "\n",
    "## 2.2 Reputation analysis on Twitter\n",
    "\n",
    "Reputation research and modeling have aroused the interest of scientists in different fields such as: sociology [15, 16], economics [17], psychology [18] and computer science [19]. Reputation analysis of celebrities based on social media Twitter usually involves number of followers, frequently used words analysis, number of likes, comments and re-tweets as metrics [20]. In our project we observed that the sentiments expressed by ordinary users to celebrities should also be considered as important metrics since they showed the attitudes of ordinary people towards celebrities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Approach\n",
    "\n",
    "Sentiment analysis is essentially a sentence classification problem (positive, negative and/or neutral) where machine learning algorithms like neural networks could be applied. In this project, we use the Bidirectional Encoder Representations from Transformers(BERT)[1] as a sentence encoder and then add a classification layer to predict the final class label.\n",
    "\n",
    "After training, the model can be applied on tweets crawled from twitter which @ a certain account to produce a \"reputation score\". we compute the scores by week and show the abosolute level as well as the relative variation of the score. \n",
    "\n",
    "## BERT with classifier\n",
    "\n",
    "BERT is a deep transformer network[6] trained on very large corpus. We use the base uncased version of BERT initialized with pretrained weight, which has 12 layers, 768 hidden unit, 12 heads, 110M parameters. The input(tweet) is preprocessed and tokenized(detail below). Tokens `[CLS]` and `[SEP]` are added to beginning and end of the input tokens respectively. The inputs are truncated or padded, depending on the length of the tokens, to a fixed length of 100, including the two added tokens. And a input mask is created with 1 indicating not padding and 0 indicating padding. Then the tokens are converted to ids using BERT vocabulary and feed to BERT with the mask. \n",
    "\n",
    "BERT produces a sentence encoding which is a 768 dimentional vector output coresponding to `[CLS]`. A dropout layer with probability of 0.1 is added to the output and then follows a fully connected layer, which outputs the logits of the labels.\n",
    "\n",
    "## Compute \"reputation score\"\n",
    "\n",
    "We crawl the tweets seperated by week that @ a certain account and preprocessed them in the same way and feed them to the model. The output of the model are the logits of the corresponding labels (positive, negative and neutral). Then we compute softmax and get probabilities. The final \"reputation score\" is computed by:\n",
    "\n",
    "$$ S = \\frac{1}{N} \\sum_{i = 1}^{N}\\left( P_i(positive) - P_i(negative) \\right) $$\n",
    "\n",
    "where $P_i(positive)$ is the probability of tweet $i$ being positive and $P_i(negative)$ is the probability of tweet $i$ being negative. $N$ is the total number of tweets crawled in one week period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Data\n",
    "We train the model on the training data from SemEval 2017 task 4A and also evaluate on the test set. The SemEval 2017 task 4A data has 50k train data with 3 labels, i.e. negative, positive and neutral.\n",
    "\n",
    "We also did experiments on Sentiment140 dataset[21] and Twitter US Airline Sentiment[22] . The Sentiment140 dataset has 1.6m data and Twitter US Airline Sentiment has 1.45k data. We split them to 90% training and 10% testing. \n",
    "\n",
    "All the data are preprocessed before inputing to the network. The data are first clean by the following:\n",
    "\n",
    "- All the @s are removed \n",
    "- Http addresses are also removed.\n",
    "- Words contain invalid ascii symbols are removed\n",
    "- All the characters that are not alphanumeric and not one of `'\"?!` are converted to a space.\n",
    "- After the above steps, tweets with less than 1 character are removed\n",
    "\n",
    "Then, the sentences are tokenized with BertTokenizer. BertTokenizer consists of basic tokenizer, which does simple spliting and converting to lower case, and a word piece tokenizer[2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Code\n",
    "\n",
    "We used the pretrained BertForSequenceClassification implementation and BertTokenizer from pytorch-pretrained-bert package. We used code from [7] to crawl tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertAdam, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from dataloader import TweetsDataset\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from util import convert_data_to_features, check_path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time, argparse\n",
    "import sklearn.metrics\n",
    "\n",
    "class TwitterSentiment():\n",
    "    def __init__(self, train_file, test_file, batch_size=16, num_epoch=10, log_interval=100, max_seq_len=100,\n",
    "                 prototype=False, parallel=False, load_model=None, num_labels=2):\n",
    "        self.log_interval = log_interval\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epoch = num_epoch\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # read training and testing data\n",
    "        self.train_data = pd.read_csv(train_file, dtype={'tag':int, 'cleaned_tweet':str})\n",
    "        self.test_data = pd.read_csv(test_file, dtype={'tag':int, 'cleaned_tweet':str})\n",
    "        if prototype:\n",
    "            self.train_data = self.train_data[:10000]\n",
    "            self.test_data = self.test_data[:1000]\n",
    "            \n",
    "        # dataloader\n",
    "        self.train_set = TweetsDataset(self.train_data, tokenizer, max_seq_len)\n",
    "        self.test_set = TweetsDataset(self.test_data, tokenizer, max_seq_len)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "        self.test_loader = DataLoader(self.test_set, batch_size=batch_size)\n",
    "\n",
    "        # build model\n",
    "        self.model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "        if parallel:\n",
    "            self.model = DataParallel(self.model)\n",
    "        if load_model:\n",
    "            self.model.load_state_dict(torch.load(load_model, map_location=self.device))\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        #optimizer\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "        t_total = int(len(self.train_data) / batch_size * num_epoch)\n",
    "        self.optimizer = BertAdam(optimizer_grouped_parameters, 2e-5, t_total=t_total)\n",
    "        # self.optimizer = optim.Adam(self.model.parameters(), lr=5e-5)\n",
    "\n",
    "    def train_epoch(self, epoch, save_interval, ckpt_file):\n",
    "        self.model.train()\n",
    "        running_ls = 0\n",
    "        acc_ls = 0\n",
    "        start = time.time()\n",
    "        num_batches = len(self.train_loader)\n",
    "        for i, batch  in enumerate(self.train_loader):\n",
    "            (input_ids, input_mask, label_ids) = tuple(t.to(self.device) for t in batch)\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            loss, _ = self.model(input_ids, attention_mask=input_mask, labels=label_ids)\n",
    "            loss.backward(torch.ones_like(loss))\n",
    "            running_ls += loss.mean().item()\n",
    "            acc_ls += loss.mean().item()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if (i+1) % self.log_interval == 0:\n",
    "                elapsed_time = time.time() - start\n",
    "                iters_per_sec = (i + 1) / elapsed_time\n",
    "                remaining = (num_batches - i - 1) / iters_per_sec\n",
    "                remaining_fmt = time.strftime(\"%H:%M:%S\", time.gmtime(remaining))\n",
    "                elapsed_fmt = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "\n",
    "                print('[{:>3}, {:>7}/{}] loss:{:.4} acc_loss:{:.4}  {:.3}iters/s {}<{}'.format(epoch, (i+1), num_batches,\n",
    "                        running_ls / self.log_interval, acc_ls/(i+1), iters_per_sec, elapsed_fmt, remaining_fmt))\n",
    "                running_ls = 0\n",
    "            if (i+1) % save_interval == 0:\n",
    "                self.save_model(ckpt_file)\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        eval_loss = 0\n",
    "        batches_count = 0\n",
    "        data_count = 0\n",
    "        pred_total = []\n",
    "        gt_total = []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(tqdm(self.test_loader)):\n",
    "                batches_count += 1\n",
    "                (input_ids, input_mask, label_ids) = tuple(t.to(self.device) for t in batch)\n",
    "                data_count += input_ids.shape[0]\n",
    "                # print(label_ids)\n",
    "                loss, logits = self.model(input_ids, attention_mask=input_mask, labels=label_ids)\n",
    "                loss = loss.mean()\n",
    "                eval_loss += loss.item()\n",
    "                logits = logits.cpu().numpy()\n",
    "                label_ids = label_ids.cpu().numpy()\n",
    "                pred = np.argmax(logits, axis=1)\n",
    "                pred_total += pred.tolist()\n",
    "                gt_total += label_ids.tolist()\n",
    "\n",
    "        eval_loss /= batches_count\n",
    "        num_correct = np.sum(np.array(pred_total) == np.array(gt_total))\n",
    "        eval_accuracy = num_correct / data_count\n",
    "        precision = sklearn.metrics.precision_score(gt_total, pred_total, average=None).mean()\n",
    "        recall = sklearn.metrics.recall_score(gt_total, pred_total, average=None).mean()\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "        print('evaluation loss: {:.4}, accuracy: {:.4}% f1 score: {:.4} AvgRec: {:.4}'.format(\n",
    "            eval_loss, eval_accuracy * 100, f1, recall))\n",
    "\n",
    "    def save_model(self, file):\n",
    "        torch.save(self.model.state_dict(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/10/2018 20:11:51 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/joeyli/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "12/10/2018 20:11:52 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/joeyli/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "12/10/2018 20:11:52 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/joeyli/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/df/sfql49b900l83087_6qy0rx00000gn/T/tmpyx79y3oh\n",
      "12/10/2018 20:11:55 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/10/2018 20:11:57 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "12/10/2018 20:11:57 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "100%|██████████| 75/75 [25:29<00:00, 20.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation loss: 0.6693, accuracy: 70.58% f1 score: 0.7029 AvgRec: 0.7066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_file = 'datastories-semeval2017-task4/train.csv'\n",
    "test_file = 'datastories-semeval2017-task4/test.csv'\n",
    "save_file = 'saved_model/bert_tweet_semeval.pth'\n",
    "check_path('saved_model')\n",
    "log_interval = 50\n",
    "num_epoch = 3\n",
    "batchsize=128\n",
    "load_model = 'saved_model/bert_tweet_semeval.pth'\n",
    "twitter_sentiment = TwitterSentiment(train_file, test_file, num_epoch=num_epoch, load_model=load_model,\n",
    "                                     batch_size=batchsize, log_interval=log_interval, prototype=False,\n",
    "                                     parallel=True, num_labels=3)\n",
    "## training\n",
    "\n",
    "# for e in range(num_epoch):\n",
    "#     twitter_sentiment.train_epoch(e+1, args.save_interval, ckpt_file)\n",
    "#     twitter_sentiment.save_model(ckpt_file)\n",
    "#     twitter_sentiment.test()\n",
    "#     twitter_sentiment.save_model('saved_model/bert_tweet_semeval_ckpt_{}.pth'.format(e+1))\n",
    "# twitter_sentiment.save_model(save_file)\n",
    "\n",
    "## testing\n",
    "\n",
    "twitter_sentiment.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Experimental Setup\n",
    "\n",
    "As mentioned above, we did experiments on SemEval 2017 task 4A and compare out method to others. We compare the average recall, F1 score and accuracy. Note that average recall is the main metric used in the task because it is a better metric for unbalanced data(e.g. Positive more that negative). So here we mainly look at average recall.\n",
    "\n",
    "And we also did experiments on Sentiment140 dataset and Twitter US Airline Sentiment. However, these datasets do not come from a shared task so there is not any comparison that can be made. We'll report our result below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Results\n",
    "\n",
    "## SemEval Result\n",
    "\n",
    "We compare our result with other SemEval 2017 participants.The best ranking teams were *BB twtr* and *DataStories*, both achieving a macro average recall of 0.681. Both top teams used deep learning; *BB twtr* used an ensemble of LSTMs and CNNs with multiple convolution operations, while *DataStories* used deep LSTM networks with an attention mechanism. As shown in the result below, our system utilizing Bert for sequence classification accieved the best result among the participant of that shared task.\n",
    "\n",
    "| #          | System        | AvgRec    | F1        | Accuracy  | Architecture                    |\n",
    "| ---------- | ------------- | --------- | --------- | --------- | -------------------------------- |\n",
    "| Our System | Windows Vista | **0.707** | **0.703** | **0.706** | Bert for Sequence Classification |\n",
    "| 1          | DataStories[3]   | 0.681     | 0.677     | 0.651     | LSTM with attention              |\n",
    "| 1          | BB twtr[4]       | 0.681     | 0.685     | 0.658     | LSTM and CNNs                    |\n",
    "| 3          | LIA[5]           | 0.676     | 0.674     | 0.661     | LSTM and CNNs                    |\n",
    "| Baseline   | All Positive  | 0.333     | 0.162     | 0.193     |                                  |\n",
    "\n",
    "\n",
    "\n",
    "## Sentiment140 and Twitter US Airline Sentiment Result\n",
    "\n",
    "These datasets do not come from a shared task so there is not any comparison that can be made. However, we here by report our test result on the test set(10%) of this dataset splited by ourselves.\n",
    "\n",
    "| dataset | Accuracy | F1 score | AvgRec |\n",
    "| ------- | -------- | -------- | ------ |\n",
    "| Sentiment140 | 0.8648 | 0.8648 | 0.8648 |\n",
    "| Twitter US Airline Sentiment | 0.8388 | 0.7919 | 0.7864 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Analysis of the Results\n",
    "\n",
    "The result on the SemEval task shows the power of BERT as a large scale pretrained transformer for language representation. Initialized with pretrained weights, tuning BERT on other tasks is simple and feasible yet produces very good results. \n",
    "\n",
    "Most existing methods use LSTM and/or CNNs. The problem with LSTM is that it is often hard to train, making it almost impossible to train a deep LSTM on very large dataset. While CNNs allieviate this problem, it turns a sequence into n-grams features and does not pay enough attention to the order. Transformer, on the other hand, does not use recurrent connections but uses many trick to maintain the ordering information as well as bidirection information flow and attention via mechanisms like multihead attention and positional embedding. In the era of deep learning, more data usually means more powerfull networks, which is exactly what BERT achieves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Reputation Analysis\n",
    "\n",
    "To evaluate the effectiveness of our reputation analysis, we visualized reputation analysis results for some typical entities. These visualizations demonstrated that our system gives satisfying results from multiple dimensions.\n",
    "\n",
    "![](images/kevin.png)\n",
    "The above graph ploted the reputation variation for Kevin Spacey, who was made a sexual assault allegation in Oct 28, 2018. The graph clearly depicted that as an actor and singer, the reputation score for Kevin was kept positive before Oct 28. However, as soon as the sexual assault news released, the reputation scores for him immediately turned into negative. This matches the time that the news published, thus for Kevin Spacey our system reasonable identified his reputation change.\n",
    "\n",
    "![](images/apple.png)\n",
    "This graph illustrated the reputation analysis for Apple Inc. Basically, both positive and negative peaks reflected some important date to the products of Apple, such as Apple events or software relase. In July, Apple silently released MacBook Pro 2018, and some positive peak can be observed. In Sep 18, Apple pushed iOS 12 to Apple devices, which overalla is not an ideal version of iOS, thus caused some negative comments on Twitter, so we can see the reputation scores reached a negative peak. However, in October Apple special event release new iPad Pro, Macbook Air and Mac Mini, which caused a series of positive comments to Apple, thus we can see a positive peak at Oct. We picked some typical tweets posted at the last two peaks, which basically reflects what happened at that time. \n",
    "\n",
    "![](images/people.png)\n",
    "This graph compares various entities from two different worlds: politcs and entertainment. As Donald Trump and Barack Obama are U.S. presidents, the reputaion scores for them are almost always negative. However, for Britney Spears and The Beatles, who come from the entertainment world, always keep positive reputation score. This matches what we usually do on Twitter: appreciating entertainment stars, but criticize political figures. \n",
    "\n",
    "![](images/sfu_ubc.png)\n",
    "Finally, this graph compared the reputation score analysis for SFU and UBC. As the plot shows, the reputation scores for both university fluctuate a lot but keep positive, and there is no overall advantage for one university. This shows the reputation for SFU and UBC are evenly matched.\n",
    "\n",
    "We evaluated the reputation analysis model from different dimensions and demonstrated its effectiveness. For figure under negative news, the big turn in reputation score clearly reflected the time in which the negative news released. For entity with multiple public events, our system general predicted public comment to the events. For entities from different worlds, our system demonstrated their general reputation are related to the world they are in. Above all, we can confidently conclude that our reputation score analysis can reflect the reputation variation towards certain entity by using sentimental model for the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Future Work\n",
    "\n",
    "Conducting sentiment analysis on twitter is a bit difficult because the language used in tweeter are informal. There are slangs, mis-spellings, abbreviations, emojis, multi-media contents and so on. A challenging task is to recognize those informal language usage and utilize them for prediction. For example, emojis are often obvious indicators of sentiment and is also ubiquitous on social media. It would be very useful if those information could be captured and utilized.\n",
    "\n",
    "Besides text contents, visual contents are also a important part of social media cotents and it also involves a lot of sentiment ques. In fact, there are already works on conbining text and visual contents for sentiment analysis on twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Jacob   Devlin,   Ming-Wei   Chang,   Kenton   Lee,   and   KristinaToutanova. Bert: Pre-training of deep bidirectional transformers forlanguage understanding.arXiv preprint arXiv:1810.04805, 2018.\n",
    "\n",
    "[2] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv:1609.08144. arXiv preprint\n",
    "\n",
    "[3] Baziotis C, Pelekis N, Doulkeridis C. Datastories at semeval-2017 task 4: Deep lstm with attention for message-level and topic-based sentiment analysis. InProceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) 2017 (pp. 747-754).\n",
    "\n",
    "[4] Mathieu Cliche. 2017. BB twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs. In Proceedings of the 11th International Workshop on Semantic Evaluation. Vancouver, Canada, SemEval ’17, pages 572–579.\n",
    "\n",
    "[5] Rouvier M. LIA at SemEval-2017 Task 4: An Ensemble of Neural Networks for Sentiment Classification. InProceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) 2017 (pp. 760-765).\n",
    "\n",
    "[6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 6000–6010.\n",
    "\n",
    "[7] https://github.com/Jefferson-Henrique/GetOldTweets-python\n",
    "\n",
    "\n",
    "[8] Rosenthal S, Nakov P, Kiritchenko S, Mohammad S, Ritter A, Stoyanov V. Semeval-2015 task 10: Sentiment analysis in twitter. InProceedings of the 9th international workshop on semantic evaluation (SemEval 2015) 2015 (pp. 451-463).\n",
    "\n",
    "[9] Nakov P, Ritter A, Rosenthal S, Sebastiani F, Stoyanov V. SemEval-2016 task 4: Sentiment analysis in Twitter. InProceedings of the 10th international workshop on semantic evaluation (semeval-2016) 2016 (pp. 1-18).\n",
    "\n",
    "[10] Cambria E, Poria S, Gelbukh A, Thelwall M. Sentiment analysis is a big suitcase. IEEE Intelligent Systems. 2017 Nov;32(6):74-80.\n",
    "\n",
    "\n",
    "[11] Soleymani M, Garcia D, Jou B, Schuller B, Chang SF, Pantic M. A survey of multimodal sentiment analysis. Image and Vision Computing. 2017 Sep 1;65:3-14.\n",
    "\n",
    "[12] Cummins N, Amiriparian S, Ottl S, Gerczuk M, Schmitt M, Schuller B. Multimodal Bag-of-Words for Cross Domains Sentiment Analysis. InProc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). Calgary, Canada 2018 (pp. 1-5).\n",
    "\n",
    "[13] Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. NRC-Canada: Building the stateof-the-art in sentiment analysis of tweets. arXiv preprint arXiv:1308.6242. 2013.\n",
    "\n",
    "[14] Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mohammad. Sentiment analysis of short informal texts. Journal of Artificial Intelligence Research 50:723–762. 2014.\n",
    "\n",
    "[15] P. Hage and F. Harary. Island Networks. Cambridge University Press, 1996.\n",
    "\n",
    "[16] V. Buskens. The social structure of trust. Social Networks, (20):265—298, 1998.\n",
    "\n",
    "[17] M. Celentani, D. Fudenberg, D.K. Levine, and W. Psendorfer. Maintaining a reputation against a long-lived opponent. Econometrica, 64(3):691—704, 1966.\n",
    "\n",
    "[18] D.B. Bromley. Reputation, Image and Impression Management. John Wiley & Sons, 1993.\n",
    "\n",
    "[19] Sabater J, Sierra C. Reputation and social network analysis in multi-agent systems. InProceedings of the first international joint conference on Autonomous agents and multiagent systems: part 1 2002 Jul 15 (pp. 475-482). ACM.\n",
    "\n",
    "[20] Snape Jennifer. Does Taylor Swift have a big (and bad) reputation? Twitter scraping using R. https://statfr.blogspot.com/2018/10/does-taylor-swift-have-big-and-bad.html. 2018.\n",
    "\n",
    "[21] http://help.sentiment140.com/for-students/\n",
    "\n",
    "[22] https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
